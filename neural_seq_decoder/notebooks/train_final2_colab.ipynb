{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final2 Model Training (Mamba-based B2T)\n",
    "\n",
    "This notebook trains the Final2 (NeuralDecoder) model with Mamba architecture.\n",
    "\n",
    "**Architecture:**\n",
    "- FeatureExtractor: Conv layers with Highway networks\n",
    "- Encoder: 5-layer bidirectional Mamba\n",
    "- Decoder: 5-layer bidirectional Mamba\n",
    "- Output: Phoneme predictions (40 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install edit-distance\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from edit_distance import SequenceMatcher\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'variant': 'final2',\n",
    "    'batchSize': 32,  # Reduced for debugging\n",
    "    'nBatch': 5000,   # Reduced for debugging\n",
    "    'seed': 0,\n",
    "    \n",
    "    # Architecture\n",
    "    'nInputFeatures': 256,\n",
    "    'nClasses': 40,\n",
    "    'conv_size': 1024,\n",
    "    'conv_kernel1': 7,\n",
    "    'conv_kernel2': 3,\n",
    "    'conv_g1': 256,\n",
    "    'conv_g2': 1,\n",
    "    'hidden_size': 512,\n",
    "    'encoder_n_layer': 5,\n",
    "    'decoder_n_layer': 5,\n",
    "    'decoders': ['ph'],  # Only phoneme decoder for simplicity\n",
    "    'update_probs': 0.7,\n",
    "    \n",
    "    # Optimizer\n",
    "    'lrStart': 0.0001,  # Very conservative learning rate\n",
    "    'lrEnd': 0.00001,\n",
    "    'l2_decay': 0.00001,\n",
    "    \n",
    "    # Augmentation\n",
    "    'whiteNoiseSD': 0.0,  # Disabled for debugging\n",
    "    'constantOffsetSD': 0.0,\n",
    "}\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_days = len(data)\n",
    "        self.n_trials = sum([len(d[\"sentenceDat\"]) for d in data])\n",
    "\n",
    "        self.neural_feats = []\n",
    "        self.phone_seqs = []\n",
    "        self.neural_time_bins = []\n",
    "        self.phone_seq_lens = []\n",
    "        self.days = []\n",
    "        \n",
    "        for day in range(self.n_days):\n",
    "            for trial in range(len(data[day][\"sentenceDat\"])):\n",
    "                self.neural_feats.append(data[day][\"sentenceDat\"][trial])\n",
    "                self.phone_seqs.append(data[day][\"phonemes\"][trial])\n",
    "                self.neural_time_bins.append(data[day][\"sentenceDat\"][trial].shape[0])\n",
    "                self.phone_seq_lens.append(data[day][\"phoneLens\"][trial])\n",
    "                self.days.append(day)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_trials\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.neural_feats[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.phone_seqs[idx], dtype=torch.int32),\n",
    "            torch.tensor(self.neural_time_bins[idx], dtype=torch.int32),\n",
    "            torch.tensor(self.phone_seq_lens[idx], dtype=torch.int32),\n",
    "            torch.tensor(self.days[idx], dtype=torch.int64),\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X, y, X_lens, y_lens, days = zip(*batch)\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    y_padded = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    return (\n",
    "        X_padded,\n",
    "        y_padded,\n",
    "        torch.stack(X_lens),\n",
    "        torch.stack(y_lens),\n",
    "        torch.stack(days),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Module Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoneme vocabulary (40 classes)\n",
    "phoneme_vocab = [\n",
    "    '|', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH',\n",
    "    'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L',\n",
    "    'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH',\n",
    "    'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH'\n",
    "]\n",
    "\n",
    "print(f\"Vocab size: {len(phoneme_vocab)} phonemes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaLayer(nn.Module):\n",
    "    \"\"\"Simplified Mamba layer\"\"\"\n",
    "    def __init__(self, d_model, expand_factor=2):\n",
    "        super(MambaLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_model * expand_factor\n",
    "\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2)\n",
    "        self.conv1d = nn.Conv1d(self.d_inner, self.d_inner, kernel_size=3, padding=1, groups=self.d_inner)\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Initialize with small values\n",
    "        nn.init.xavier_uniform_(self.in_proj.weight, gain=0.01)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight, gain=0.01)\n",
    "        nn.init.zeros_(self.in_proj.bias)\n",
    "        nn.init.zeros_(self.out_proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x_proj = self.in_proj(x)\n",
    "        x, gate = x_proj.chunk(2, dim=-1)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.activation(x) * torch.sigmoid(gate)\n",
    "        x = self.out_proj(x)\n",
    "        return x + residual\n",
    "\n",
    "class mamba_block(nn.Module):\n",
    "    def __init__(self, d_model, n_layer=1, bidirectional=False, update_probs=0.7):\n",
    "        super(mamba_block, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layer = n_layer\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.forward_layers = nn.ModuleList([MambaLayer(d_model) for _ in range(n_layer)])\n",
    "        if bidirectional:\n",
    "            self.backward_layers = nn.ModuleList([MambaLayer(d_model) for _ in range(n_layer)])\n",
    "        self.dropout = nn.Dropout(1.0 - update_probs) if update_probs < 1.0 else None\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        for layer in self.forward_layers:\n",
    "            x = layer(x)\n",
    "            if self.dropout is not None:\n",
    "                x = self.dropout(x)\n",
    "        if self.bidirectional:\n",
    "            x_backward = torch.flip(x, dims=[1])\n",
    "            for layer in self.backward_layers:\n",
    "                x_backward = layer(x_backward)\n",
    "                if self.dropout is not None:\n",
    "                    x_backward = self.dropout(x_backward)\n",
    "            x_backward = torch.flip(x_backward, dims=[1])\n",
    "            x = (x + x_backward) / 2\n",
    "        return x, lens\n",
    "\n",
    "print(\"✓ Mamba modules defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=1):\n",
    "        super(Highway, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = nn.ModuleDict({\n",
    "                'transform': nn.Linear(input_dim, input_dim),\n",
    "                'gate': nn.Linear(input_dim, input_dim),\n",
    "            })\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        for layer in self.layers:\n",
    "            transform_gate = torch.sigmoid(layer['gate'](x))\n",
    "            transform = torch.relu(layer['transform'](x))\n",
    "            x = transform_gate * transform + (1 - transform_gate) * x\n",
    "        return x, lens\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims, kernel_size, stride, groups=1):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv1d(input_dims, output_dims, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
    "        self.bn = nn.BatchNorm1d(output_dims)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        new_lens = (lens + 2 * ((self.kernel_size - 1) // 2) - self.kernel_size) // self.stride + 1\n",
    "        new_lens = new_lens.clamp(min=1)\n",
    "        return x, new_lens\n",
    "\n",
    "class UnPack(nn.Module):\n",
    "    def forward(self, x, lens):\n",
    "        return x, lens\n",
    "\n",
    "print(\"✓ Support modules defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleStack(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(ModuleStack, self).__init__()\n",
    "        modules_list = []\n",
    "        self.output_dims = 0\n",
    "\n",
    "        for l in layers:\n",
    "            if l[0] == \"mamba\":\n",
    "                _, in_channels, n_layer, bidirectional, update_probs = l\n",
    "                modules_list.append(mamba_block(d_model=in_channels, n_layer=n_layer, bidirectional=bidirectional, update_probs=update_probs))\n",
    "                self.output_dims = in_channels\n",
    "            elif l[0] == \"highway\":\n",
    "                _, in_channels, n_layer = l\n",
    "                modules_list.append(Highway(input_dim=in_channels, num_layers=n_layer))\n",
    "                self.output_dims = in_channels\n",
    "            elif l[0] == \"conv\":\n",
    "                _, input_dims, output_dims, kernel_size, stride, groups = l\n",
    "                modules_list.append(conv_block(input_dims, output_dims, kernel_size, stride, groups))\n",
    "                self.output_dims = output_dims\n",
    "            elif l[0] == \"unpack\":\n",
    "                modules_list.append(UnPack())\n",
    "\n",
    "        self.layers = nn.ModuleList(modules_list)\n",
    "        assert self.output_dims != 0\n",
    "\n",
    "    def forward(self, hidden_states, lens):\n",
    "        for layer in self.layers:\n",
    "            hidden_states, lens = layer(hidden_states, lens)\n",
    "        return hidden_states, lens\n",
    "\n",
    "print(\"✓ ModuleStack defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralDecoder(nn.Module):\n",
    "    \"\"\"Simplified NeuralDecoder for Colab\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SimpleNeuralDecoder, self).__init__()\n",
    "        \n",
    "        # Feature extractor\n",
    "        self.feature_extractor = ModuleStack([\n",
    "            [\"unpack\"],\n",
    "            [\"conv\", 256, config['conv_size'], config['conv_kernel1'], 2, config['conv_g1']],\n",
    "            [\"highway\", config['conv_size'], 2],\n",
    "            [\"conv\", config['conv_size'], config['hidden_size'], config['conv_kernel2'], 2, config['conv_g2']],\n",
    "            [\"highway\", config['hidden_size'], 2],\n",
    "        ])\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = ModuleStack([\n",
    "            [\"mamba\", config['hidden_size'], config['encoder_n_layer'], True, config['update_probs']]\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = ModuleStack([\n",
    "            [\"mamba\", config['hidden_size'], config['decoder_n_layer'], True, config['update_probs']]\n",
    "        ])\n",
    "        \n",
    "        # Output layer (40 phonemes + 1 blank for CTC)\n",
    "        self.linear = nn.Linear(config['hidden_size'], config['nClasses'] + 1)\n",
    "        \n",
    "    def forward(self, x, day_idx=None):\n",
    "        batch_size = x.shape[0]\n",
    "        input_lens = torch.full((batch_size,), x.shape[1], device=x.device, dtype=torch.int32)\n",
    "        \n",
    "        # Feature extraction\n",
    "        x, lens = self.feature_extractor(x, input_lens)\n",
    "        \n",
    "        # Encoder\n",
    "        x, lens = self.encoder(x, lens)\n",
    "        \n",
    "        # Decoder\n",
    "        x, lens = self.decoder(x, lens)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        # Store lens for CTC loss\n",
    "        self._last_output_lens = lens\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"✓ SimpleNeuralDecoder defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Update this path to your data location\n",
    "# For Colab, upload the data first or mount Google Drive\n",
    "DATASET_PATH = \"/home/ivansit1214/competitionData/ptDecoder_ctc\"  # Update this!\n",
    "\n",
    "# For Google Drive (uncomment if using Drive):\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATASET_PATH = '/content/drive/MyDrive/competitionData/ptDecoder_ctc'\n",
    "\n",
    "print(f\"Loading data from: {DATASET_PATH}\")\n",
    "with open(DATASET_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_ds = SpeechDataset(data[\"train\"])\n",
    "test_ds = SpeechDataset(data[\"test\"])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config['batchSize'], shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=config['batchSize'], shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_ds)}\")\n",
    "print(f\"✓ Test samples: {len(test_ds)}\")\n",
    "print(f\"✓ Number of days: {len(data['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "\n",
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model = SimpleNeuralDecoder(config).to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"✓ Total parameters: {total_params:,}\")\n",
    "print(f\"✓ Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_ctc = nn.CTCLoss(blank=0, reduction=\"mean\", zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lrStart'], weight_decay=config['l2_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=config['lrEnd']/config['lrStart'], total_iters=config['nBatch'])\n",
    "\n",
    "print(f\"✓ Optimizer: Adam, lr={config['lrStart']}\")\n",
    "print(f\"✓ Model ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Forward Pass (DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with one batch\n",
    "print(\"Testing forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X, y, X_len, y_len, day_idx = next(iter(train_loader))\n",
    "    X = X.to(DEVICE)\n",
    "    \n",
    "    print(f\"Input shape: {X.shape}\")\n",
    "    pred = model(X)\n",
    "    print(f\"Output shape: {pred.shape}\")\n",
    "    print(f\"Output lens: {model._last_output_lens}\")\n",
    "    print(f\"Output min: {pred.min().item():.4f}, max: {pred.max().item():.4f}\")\n",
    "    print(f\"✓ Forward pass works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting training for {config['nBatch']} batches...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_loss_list = []\n",
    "test_per_list = []\n",
    "best_per = None\n",
    "start_time = time.time()\n",
    "\n",
    "for batch_idx in range(config['nBatch']):\n",
    "    model.train()\n",
    "    \n",
    "    # Get batch\n",
    "    X, y, X_len, y_len, day_idx = next(iter(train_loader))\n",
    "    X, y, X_len, y_len = X.to(DEVICE), y.to(DEVICE), X_len.to(DEVICE), y_len.to(DEVICE)\n",
    "    \n",
    "    # Forward\n",
    "    pred = model(X)\n",
    "    out_lens = model._last_output_lens\n",
    "    \n",
    "    # CTC loss\n",
    "    loss = loss_ctc(\n",
    "        torch.permute(pred.log_softmax(2), [1, 0, 2]),\n",
    "        y,\n",
    "        out_lens,\n",
    "        y_len,\n",
    "    )\n",
    "    loss = torch.sum(loss)\n",
    "    \n",
    "    # Check for NaN\n",
    "    if torch.isnan(loss):\n",
    "        print(f\"\\n⚠️  NaN loss at batch {batch_idx}!\")\n",
    "        print(f\"   pred stats: min={pred.min():.4f}, max={pred.max():.4f}, mean={pred.mean():.4f}\")\n",
    "        print(f\"   Stopping training...\")\n",
    "        break\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Evaluation every 100 batches\n",
    "    if batch_idx % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            all_loss = []\n",
    "            total_edit = 0\n",
    "            total_len = 0\n",
    "            \n",
    "            for eval_idx, (X, y, X_len, y_len, test_day_idx) in enumerate(test_loader):\n",
    "                if eval_idx >= 10:  # Only 10 batches for speed\n",
    "                    break\n",
    "                    \n",
    "                X, y, X_len, y_len = X.to(DEVICE), y.to(DEVICE), X_len.to(DEVICE), y_len.to(DEVICE)\n",
    "                pred = model(X)\n",
    "                out_lens = model._last_output_lens\n",
    "                \n",
    "                loss = loss_ctc(torch.permute(pred.log_softmax(2), [1, 0, 2]), y, out_lens, y_len)\n",
    "                loss = torch.sum(loss)\n",
    "                all_loss.append(loss.cpu().detach().numpy())\n",
    "                \n",
    "                # Decode and compute PER\n",
    "                for i in range(pred.shape[0]):\n",
    "                    logits = pred[i, :out_lens[i], :]\n",
    "                    decoded = torch.argmax(logits, dim=-1)\n",
    "                    decoded = torch.unique_consecutive(decoded)\n",
    "                    decoded = decoded.cpu().detach().numpy()\n",
    "                    decoded = decoded[decoded != 0]\n",
    "                    target = y[i, :y_len[i]].cpu().detach().numpy()\n",
    "                    matcher = SequenceMatcher(a=target.tolist(), b=decoded.tolist())\n",
    "                    total_edit += matcher.distance()\n",
    "                    total_len += len(target)\n",
    "            \n",
    "            avg_loss = np.mean(all_loss)\n",
    "            per = total_edit / total_len\n",
    "            elapsed = (time.time() - start_time) / 100 if batch_idx > 0 else 0.0\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"batch {batch_idx:5d}, loss: {avg_loss:.4f}, PER: {per:.4f}, lr: {current_lr:.6f}, time/batch: {elapsed:.3f}s\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            test_loss_list.append(avg_loss)\n",
    "            test_per_list.append(per)\n",
    "            \n",
    "            if best_per is None or per < best_per:\n",
    "                best_per = per\n",
    "                print(f\"  → New best PER: {per:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "if best_per is not None:\n",
    "    print(f\"Best PER: {best_per:.4f} ({best_per*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(test_loss_list)\n",
    "ax1.set_xlabel('Evaluation Step (x100 batches)')\n",
    "ax1.set_ylabel('Test Loss')\n",
    "ax1.set_title('Test Loss Over Training')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(test_per_list)\n",
    "ax2.set_xlabel('Evaluation Step (x100 batches)')\n",
    "ax2.set_ylabel('Phoneme Error Rate')\n",
    "ax2.set_title('PER Over Training')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final PER: {test_per_list[-1]:.4f}\" if test_per_list else \"No results yet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
