{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language Model Phoneme Evaluation\n",
        "\n",
        "This notebook evaluates trained models using a phoneme language model for improved accuracy.\n",
        "\n",
        "**What this does:**\n",
        "- Loads a trained model (GRU-Opt, Final2/Mamba, or Transformer)\n",
        "- Runs baseline greedy decoding\n",
        "- Runs beam search with phoneme language model\n",
        "- Compares results and shows improvement\n",
        "\n",
        "**Expected improvement:** 2-8 percentage points lower PER/CER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'src')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "from edit_distance import SequenceMatcher\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Update these paths for your setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== UPDATE THESE PATHS =====\n",
        "MODEL_TYPE = \"final\"  # Options: \"final\", \"final2\", \"advanced\"\n",
        "MODEL_WEIGHTS = os.path.expanduser(\"~/results/final_training/modelWeights.pt\")\n",
        "DATASET_PATH = os.path.expanduser(\"~/competitionData/ptDecoder_ctc\")\n",
        "LM_PATH = \"phoneme_lm.arpa\"  # In notebooks folder\n",
        "PHONEME_MAP_PATH = \"phoneme_map.txt\"  # In notebooks folder\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# LM settings (can adjust)\n",
        "LM_WEIGHT = 0.8  # 0.4-1.2 (higher = trust LM more)\n",
        "BEAM_WIDTH = 10  # 5-50 (higher = slower but more accurate)\n",
        "\n",
        "print(f\"Model: {MODEL_TYPE}\")\n",
        "print(f\"Weights: {MODEL_WEIGHTS}\")\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install KenLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import kenlm\n",
        "    print(\"✓ KenLM already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing KenLM...\")\n",
        "    !pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "    import kenlm\n",
        "    print(\"✓ KenLM installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neural_decoder.dataset import SpeechDataset\n",
        "\n",
        "with open(DATASET_PATH, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "test_ds = SpeechDataset(data[\"test\"])  # Actually validation set\n",
        "\n",
        "def collate_fn(batch):\n",
        "    X, y, X_lens, y_lens, days = zip(*batch)\n",
        "    X_padded = pad_sequence(X, batch_first=True, padding_value=0.0)\n",
        "    y_padded = pad_sequence(y, batch_first=True, padding_value=0)\n",
        "    return (\n",
        "        X_padded,\n",
        "        y_padded,\n",
        "        torch.stack(X_lens),\n",
        "        torch.stack(y_lens),\n",
        "        torch.stack(days),\n",
        "    )\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"✓ Loaded {len(test_ds)} validation samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODEL_TYPE == \"final\":\n",
        "    from neural_decoder.model import GRUDecoder\n",
        "    model = GRUDecoder(\n",
        "        neural_dim=256,\n",
        "        n_classes=40,\n",
        "        hidden_dim=1024,\n",
        "        layer_dim=5,\n",
        "        nDays=len(data[\"train\"]),\n",
        "        dropout=0.4,\n",
        "        device=DEVICE,\n",
        "        strideLen=4,\n",
        "        kernelLen=32,\n",
        "        gaussianSmoothWidth=2.0,\n",
        "        bidirectional=True,\n",
        "    ).to(DEVICE)\n",
        "    stride_len = 4\n",
        "    kernel_len = 32\n",
        "\n",
        "elif MODEL_TYPE == \"final2\":\n",
        "    from neural_decoder.final2_model import NeuralDecoder\n",
        "    model = NeuralDecoder(\n",
        "        conv_size=1024,\n",
        "        hidden_size=1024,\n",
        "        encoder_n_layer=8,\n",
        "        decoder_n_layer=8,\n",
        "        decoders=['ph'],\n",
        "    ).to(DEVICE)\n",
        "    stride_len = 4\n",
        "    kernel_len = 8\n",
        "\n",
        "elif MODEL_TYPE == \"advanced\":\n",
        "    from neural_decoder.advanced_models import StreamingTransformerDecoder\n",
        "    model = StreamingTransformerDecoder(\n",
        "        neural_dim=256,\n",
        "        n_phonemes=40,\n",
        "        d_model=512,\n",
        "        nhead=8,\n",
        "        num_layers=6,\n",
        "        dropout=0.2,\n",
        "        stride_len=4,\n",
        "        kernel_len=8,\n",
        "        gaussian_smooth_width=2.0,\n",
        "        intermediate_layer=3,\n",
        "        day_count=len(data[\"train\"]),\n",
        "        diphone_context=40,\n",
        "        device=DEVICE,\n",
        "    ).to(DEVICE)\n",
        "    stride_len = 8  # Transformer uses patch embedding\n",
        "    kernel_len = 8\n",
        "\n",
        "# Load weights\n",
        "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"✓ Loaded {MODEL_TYPE} model ({total_params/1e6:.1f}M parameters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PhonemeLM:\n",
        "    def __init__(self, lm_path: str, phoneme_map: dict = None):\n",
        "        if not os.path.exists(lm_path):\n",
        "            raise FileNotFoundError(f\"LM file not found: {lm_path}\")\n",
        "        self.model = kenlm.Model(lm_path)\n",
        "        self.phoneme_map = phoneme_map or {}\n",
        "        self._score_cache = {}\n",
        "\n",
        "    def id_sequence_to_tokens(self, id_seq):\n",
        "        tokens = []\n",
        "        for i in id_seq:\n",
        "            t = self.phoneme_map.get(int(i), None)\n",
        "            if t is None:\n",
        "                t = f\"PH{int(i)}\"\n",
        "            tokens.append(t)\n",
        "        return tokens\n",
        "\n",
        "    def score(self, id_seq):\n",
        "        key = tuple(id_seq)\n",
        "        if key in self._score_cache:\n",
        "            return self._score_cache[key]\n",
        "        tokens = self.id_sequence_to_tokens(id_seq)\n",
        "        s = self.model.score(\" \".join(tokens), bos=False, eos=False)\n",
        "        self._score_cache[key] = float(s)\n",
        "        return float(s)\n",
        "\n",
        "    def clear_cache(self):\n",
        "        self._score_cache.clear()\n",
        "\n",
        "def load_phoneme_map(path, n_classes):\n",
        "    if path is None or not os.path.exists(path):\n",
        "        return {i: f\"PH{i}\" for i in range(1, n_classes + 1)}\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [l.strip() for l in f if l.strip()]\n",
        "            m = {}\n",
        "            for idx, tok in enumerate(lines, start=1):\n",
        "                m[idx] = tok\n",
        "            for i in range(1, n_classes + 1):\n",
        "                if i not in m:\n",
        "                    m[i] = f\"PH{i}\"\n",
        "            return m\n",
        "    except Exception:\n",
        "        return {i: f\"PH{i}\" for i in range(1, n_classes + 1)}\n",
        "\n",
        "phoneme_map = load_phoneme_map(PHONEME_MAP_PATH, 40)\n",
        "lm = PhonemeLM(LM_PATH, phoneme_map=phoneme_map)\n",
        "\n",
        "print(f\"✓ Language model loaded from: {LM_PATH}\")\n",
        "print(f\"  LM Weight: {LM_WEIGHT}\")\n",
        "print(f\"  Beam Width: {BEAM_WIDTH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beam Search Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def beam_search_lm(log_probs, lm_wrapper=None, lm_weight=0.8, beam_width=10, blank_id=0, topk_acoustic=5):\n",
        "    if isinstance(log_probs, torch.Tensor):\n",
        "        lp = log_probs.detach().cpu().numpy()\n",
        "    else:\n",
        "        lp = np.array(log_probs)\n",
        "    T, V = lp.shape\n",
        "\n",
        "    beams = [([], 0.0, 0.0)]  # (sequence, acoustic_score, lm_score)\n",
        "    for t in range(T):\n",
        "        step = lp[t]\n",
        "        topk_idx = np.argsort(step)[-topk_acoustic:][::-1]\n",
        "        new_beams = {}\n",
        "        for seq, a_score, l_score in beams:\n",
        "            for idx in topk_idx:\n",
        "                token_logp = float(step[idx])\n",
        "                if idx == blank_id:\n",
        "                    new_seq = tuple(seq)\n",
        "                    new_a = a_score + token_logp\n",
        "                    new_l = l_score\n",
        "                else:\n",
        "                    new_seq = tuple(list(seq) + [int(idx)])\n",
        "                    new_a = a_score + token_logp\n",
        "                    if lm_wrapper is not None:\n",
        "                        new_l = lm_wrapper.score(new_seq)\n",
        "                    else:\n",
        "                        new_l = 0.0\n",
        "                combined = new_a + (lm_weight * new_l)\n",
        "                if new_seq not in new_beams or combined > new_beams[new_seq][0]:\n",
        "                    new_beams[new_seq] = (combined, new_a, new_l)\n",
        "        sorted_beams = sorted(new_beams.items(), key=lambda x: x[1][0], reverse=True)[:beam_width]\n",
        "        beams = [(list(k), v[1], v[2]) for k, v in sorted_beams]\n",
        "\n",
        "    best = max(beams, key=lambda b: b[1] + lm_weight * b[2])\n",
        "    decoded = best[0]\n",
        "    collapsed = []\n",
        "    prev = None\n",
        "    for tok in decoded:\n",
        "        if tok == prev:\n",
        "            prev = tok\n",
        "            continue\n",
        "        if tok != blank_id:\n",
        "            collapsed.append(tok)\n",
        "        prev = tok\n",
        "    return collapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"EVALUATING WITH LANGUAGE MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_predictions_baseline = []\n",
        "all_predictions_lm = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (X, y, X_len, y_len, day_idx) in enumerate(test_loader):\n",
        "        X = X.to(DEVICE)\n",
        "        X_len = X_len.to(DEVICE)\n",
        "        day_idx = day_idx.to(DEVICE)\n",
        "        \n",
        "        # Forward pass (model-specific)\n",
        "        if MODEL_TYPE == \"final\":\n",
        "            logits = model(X, day_idx)\n",
        "            lengths = ((X_len - kernel_len) / stride_len).long()\n",
        "            log_probs = torch.log_softmax(logits, dim=-1)\n",
        "        elif MODEL_TYPE == \"final2\":\n",
        "            logits = model(X, day_idx)\n",
        "            lengths = model._last_output_lens if hasattr(model, '_last_output_lens') else ((X_len - kernel_len) / stride_len).long()\n",
        "            log_probs = torch.log_softmax(logits, dim=-1)\n",
        "        elif MODEL_TYPE == \"advanced\":\n",
        "            output = model(X, X_len, day_idx)\n",
        "            log_probs = output['log_probs']\n",
        "            lengths = output['eff_lengths']\n",
        "        \n",
        "        # Decode each sample\n",
        "        for i in range(len(y)):\n",
        "            seq_len = int(lengths[i])\n",
        "            lp = log_probs[i, :seq_len, :]  # [T, V]\n",
        "            \n",
        "            # Baseline: Greedy decoding\n",
        "            greedy = torch.argmax(lp, dim=-1).cpu().numpy()\n",
        "            decoded_baseline = []\n",
        "            prev = None\n",
        "            for tok in greedy:\n",
        "                if tok == prev or tok == 0:\n",
        "                    prev = tok\n",
        "                    continue\n",
        "                decoded_baseline.append(tok)\n",
        "                prev = tok\n",
        "            all_predictions_baseline.append(decoded_baseline)\n",
        "            \n",
        "            # With LM: Beam search\n",
        "            decoded_lm = beam_search_lm(\n",
        "                lp,\n",
        "                lm_wrapper=lm,\n",
        "                lm_weight=LM_WEIGHT,\n",
        "                beam_width=BEAM_WIDTH,\n",
        "                blank_id=0,\n",
        "                topk_acoustic=5\n",
        "            )\n",
        "            all_predictions_lm.append(decoded_lm)\n",
        "            \n",
        "            # Get target\n",
        "            target = y[i, :y_len[i]].cpu().numpy().tolist()\n",
        "            all_targets.append(target)\n",
        "        \n",
        "        if (batch_idx + 1) % 5 == 0:\n",
        "            print(f\"  Processed {batch_idx + 1}/{len(test_loader)} batches\")\n",
        "\n",
        "print(\"✓ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_error_rate(predictions, targets):\n",
        "    total_edit = 0\n",
        "    total_len = 0\n",
        "    for pred, target in zip(predictions, targets):\n",
        "        matcher = SequenceMatcher(a=target, b=pred)\n",
        "        total_edit += matcher.distance()\n",
        "        total_len += len(target)\n",
        "    return total_edit / total_len if total_len > 0 else 0.0\n",
        "\n",
        "cer_baseline = compute_error_rate(all_predictions_baseline, all_targets)\n",
        "cer_lm = compute_error_rate(all_predictions_lm, all_targets)\n",
        "improvement = (cer_baseline - cer_lm) / cer_baseline * 100\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nModel: {MODEL_TYPE}\")\n",
        "print(f\"\\nBaseline (Greedy Decoding):\")\n",
        "print(f\"  PER/CER: {cer_baseline:.4f} ({cer_baseline*100:.2f}%)\")\n",
        "print(f\"\\nWith Language Model:\")\n",
        "print(f\"  PER/CER: {cer_lm:.4f} ({cer_lm*100:.2f}%)\")\n",
        "print(f\"  Improvement: {improvement:.2f}% relative\")\n",
        "print(f\"  Absolute gain: {(cer_baseline - cer_lm)*100:.2f} percentage points\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"Language Model improved accuracy by {improvement:.1f}%!\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
